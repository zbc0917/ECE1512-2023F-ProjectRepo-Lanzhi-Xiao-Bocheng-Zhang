# Project B

## Supplementatary File

Here is the explanation of what is included in the Supplementary file:


    1.networks.ipynb is a Python notebook that provides six different networks, including MLP, Convent, LeNet, AlexNet, VGG11, and ResNet-18. You will use this file to learn synthetic dates on the MNIST and CIFAR10 datasets.

    2. mhist_dataset is a folder containing two subfolders, including the images and their annotations, for the MHIST dataset. All 3152 images are in the images.zip file. Annotations are included in annotations.csv. Note that this file includes each image file name and its corresponding majority-vote label and degree of annotator agreement expressed as the number of annotators who marked the image as SSA (e.g., 6 indicates 6/7 agreement with a ground truth of SSA, and 2 would indicate 5/7 agreement with a ground truth of HP).
    
    3.utils.ipynb is a Python notebook that provides utilities such as access to datasets, preprocessing, and data augmentation. You will use this file to learn synthetic dates on the MNIST and CIFAR10 datasets.

    4. Project_B_FAQs.pdf is a list of frequently asked questions that try to shed light on (almost) all of your questions and concerns that you may have during Project B.

    5. Task1_MNIST.ipynb is a python notebook that using the original dataset and synthetic dataset (MNIST) to train the models, ConvNet, AlexNet, VGG11. The synthetic dataset will be created by the Dataset distillation with gradient matching method. (Inspired by: https://arxiv.org/abs/2006.05929)

    6. Task1_MHIST.ipynb is a python notebook that using the original dataset and synthetic dataset (MHIST) to train the models, ConvNet, AlexNet, VGG11. The synthetic dataset will be created by the Dataset distillation with gradient matching method. (Inspired by: https://arxiv.org/abs/2006.05929

    7. Task1_4.ipynb is a python notebook that apply synthetic small datasets to federated learning by integrating the concept of dataset distillation.

    8. Task2_DM.ipynb is a python notebook that apply the dataset condensation with distribution matching method to create a faster set of synthetic dataset than GM.

The following file inspired by Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. Towards lossless dataset distillation via difficulty-aligned trajectory matching. arXiv preprint arXiv:2310.05773, 2023.https://arxiv.org/pdf/2310.05773.pdf:

    9. Task2_GATM.ipyb is a python notebook that apply the Difficulty-Aligned Trajectory Matching to create a lossless synthetic dataset than GM. 

    10. task2_gatm.py is a python copy fo the Task2_GATM. 

    11. reparam_module is a python notebook that provid a reparam model. 

    12. MNISTbuffer is a python notebook that provides a bufferfile of MNIST Dataset. 

Report:

    13. ECE1512_B_Report.pdf is a pdf file introducing and summarize the finding of the project B.

