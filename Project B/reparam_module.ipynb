{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asLlToISX3KY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@misc{guo2023lossless,\n",
        "  title={Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching},\n",
        "  author={Ziyao Guo and Kai Wang and George Cazenavette and Hui Li and Kaipeng Zhang and Yang You},\n",
        "  year={2023},\n",
        "  eprint={2310.05773},\n",
        "  archivePrefix={arXiv},\n",
        "  primaryClass={cs.CV}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "import types\n",
        "from collections import namedtuple\n",
        "from contextlib import contextmanager\n",
        "\n",
        "\n",
        "class ReparamModule(nn.Module):\n",
        "    def _get_module_from_name(self, mn):\n",
        "        if mn == '':\n",
        "            return self\n",
        "        m = self\n",
        "        for p in mn.split('.'):\n",
        "            m = getattr(m, p)\n",
        "        return m\n",
        "\n",
        "    def __init__(self, module):\n",
        "        super(ReparamModule, self).__init__()\n",
        "        self.module = module\n",
        "\n",
        "        param_infos = []  # (module name/path, param name)\n",
        "        shared_param_memo = {}\n",
        "        shared_param_infos = []  # (module name/path, param name, src module name/path, src param_name)\n",
        "        params = []\n",
        "        param_numels = []\n",
        "        param_shapes = []\n",
        "        for mn, m in self.named_modules():\n",
        "            for n, p in m.named_parameters(recurse=False):\n",
        "                if p is not None:\n",
        "                    if p in shared_param_memo:\n",
        "                        shared_mn, shared_n = shared_param_memo[p]\n",
        "                        shared_param_infos.append((mn, n, shared_mn, shared_n))\n",
        "                    else:\n",
        "                        shared_param_memo[p] = (mn, n)\n",
        "                        param_infos.append((mn, n))\n",
        "                        params.append(p.detach())\n",
        "                        param_numels.append(p.numel())\n",
        "                        param_shapes.append(p.size())\n",
        "\n",
        "        assert len(set(p.dtype for p in params)) <= 1, \\\n",
        "            \"expects all parameters in module to have same dtype\"\n",
        "\n",
        "        # store the info for unflatten\n",
        "        self._param_infos = tuple(param_infos)\n",
        "        self._shared_param_infos = tuple(shared_param_infos)\n",
        "        self._param_numels = tuple(param_numels)\n",
        "        self._param_shapes = tuple(param_shapes)\n",
        "\n",
        "        # flatten\n",
        "        flat_param = nn.Parameter(torch.cat([p.reshape(-1) for p in params], 0))\n",
        "        self.register_parameter('flat_param', flat_param)\n",
        "        self.param_numel = flat_param.numel()\n",
        "        del params\n",
        "        del shared_param_memo\n",
        "\n",
        "        # deregister the names as parameters\n",
        "        for mn, n in self._param_infos:\n",
        "            delattr(self._get_module_from_name(mn), n)\n",
        "        for mn, n, _, _ in self._shared_param_infos:\n",
        "            delattr(self._get_module_from_name(mn), n)\n",
        "\n",
        "        # register the views as plain attributes\n",
        "        self._unflatten_param(self.flat_param)\n",
        "\n",
        "        # now buffers\n",
        "        # they are not reparametrized. just store info as (module, name, buffer)\n",
        "        buffer_infos = []\n",
        "        for mn, m in self.named_modules():\n",
        "            for n, b in m.named_buffers(recurse=False):\n",
        "                if b is not None:\n",
        "                    buffer_infos.append((mn, n, b))\n",
        "\n",
        "        self._buffer_infos = tuple(buffer_infos)\n",
        "        self._traced_self = None\n",
        "\n",
        "    def trace(self, example_input, **trace_kwargs):\n",
        "        assert self._traced_self is None, 'This ReparamModule is already traced'\n",
        "\n",
        "        if isinstance(example_input, torch.Tensor):\n",
        "            example_input = (example_input,)\n",
        "        example_input = tuple(example_input)\n",
        "        example_param = (self.flat_param.detach().clone(),)\n",
        "        example_buffers = (tuple(b.detach().clone() for _, _, b in self._buffer_infos),)\n",
        "\n",
        "        self._traced_self = torch.jit.trace_module(\n",
        "            self,\n",
        "            inputs=dict(\n",
        "                _forward_with_param=example_param + example_input,\n",
        "                _forward_with_param_and_buffers=example_param + example_buffers + example_input,\n",
        "            ),\n",
        "            **trace_kwargs,\n",
        "        )\n",
        "\n",
        "        # replace forwards with traced versions\n",
        "        self._forward_with_param = self._traced_self._forward_with_param\n",
        "        self._forward_with_param_and_buffers = self._traced_self._forward_with_param_and_buffers\n",
        "        return self\n",
        "\n",
        "    def clear_views(self):\n",
        "        for mn, n in self._param_infos:\n",
        "            setattr(self._get_module_from_name(mn), n, None)  # This will set as plain attr\n",
        "\n",
        "    def _apply(self, *args, **kwargs):\n",
        "        if self._traced_self is not None:\n",
        "            self._traced_self._apply(*args, **kwargs)\n",
        "            return self\n",
        "        return super(ReparamModule, self)._apply(*args, **kwargs)\n",
        "\n",
        "    def _unflatten_param(self, flat_param):\n",
        "        ps = (t.view(s) for (t, s) in zip(flat_param.split(self._param_numels), self._param_shapes))\n",
        "        for (mn, n), p in zip(self._param_infos, ps):\n",
        "            setattr(self._get_module_from_name(mn), n, p)  # This will set as plain attr\n",
        "        for (mn, n, shared_mn, shared_n) in self._shared_param_infos:\n",
        "            setattr(self._get_module_from_name(mn), n, getattr(self._get_module_from_name(shared_mn), shared_n))\n",
        "\n",
        "    @contextmanager\n",
        "    def unflattened_param(self, flat_param):\n",
        "        saved_views = [getattr(self._get_module_from_name(mn), n) for mn, n in self._param_infos]\n",
        "        self._unflatten_param(flat_param)\n",
        "        yield\n",
        "        for (mn, n), p in zip(self._param_infos, saved_views):\n",
        "            setattr(self._get_module_from_name(mn), n, p)\n",
        "        for (mn, n, shared_mn, shared_n) in self._shared_param_infos:\n",
        "            setattr(self._get_module_from_name(mn), n, getattr(self._get_module_from_name(shared_mn), shared_n))\n",
        "\n",
        "    @contextmanager\n",
        "    def replaced_buffers(self, buffers):\n",
        "        for (mn, n, _), new_b in zip(self._buffer_infos, buffers):\n",
        "            setattr(self._get_module_from_name(mn), n, new_b)\n",
        "        yield\n",
        "        for mn, n, old_b in self._buffer_infos:\n",
        "            setattr(self._get_module_from_name(mn), n, old_b)\n",
        "\n",
        "    def _forward_with_param_and_buffers(self, flat_param, buffers, *inputs, **kwinputs):\n",
        "        with self.unflattened_param(flat_param):\n",
        "            with self.replaced_buffers(buffers):\n",
        "                return self.module(*inputs, **kwinputs)\n",
        "\n",
        "    def _forward_with_param(self, flat_param, *inputs, **kwinputs):\n",
        "        with self.unflattened_param(flat_param):\n",
        "            return self.module(*inputs, **kwinputs)\n",
        "\n",
        "    def forward(self, *inputs, flat_param=None, buffers=None, **kwinputs):\n",
        "        flat_param = torch.squeeze(flat_param)\n",
        "        if flat_param is None:\n",
        "            flat_param = self.flat_param\n",
        "        if buffers is None:\n",
        "            return self._forward_with_param(flat_param, *inputs, **kwinputs)\n",
        "        else:\n",
        "            return self._forward_with_param_and_buffers(flat_param, tuple(buffers), *inputs, **kwinputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hE_otXMxYKXN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}